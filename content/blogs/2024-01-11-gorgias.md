---
title: ""
description: ""
date: 2024-01-11T09:00:00
category: Solutions
author:
  name: Benoit Pimpaud
  image: "bpimpaud"
image: /blogs/2024-01-11-gorgias.jpg
---

Infrastructure as Code (IaC) has become the industry standard for cloud resources management and configuration.

It allows teams to easily collaborate across multiple resources through providers and have a common declarative syntax for several purposes: IAM policies, Kubernetes clusters, database users and tables, etc.

What if this same principle were applied to Data Engineering?
That's what [Gorgias](https://www.gorgias.com/) is doing with [Kestra](https://github.com/kestra-io/kestra).

With the rise of the Modern Data Stack, workflows span across diverse tools — ranging from ETL scripts, data transformations, and databases to reverse-ETL processes and ML models.

The current state of solutions involves scattering configurations across various domains:

* Infrastructure: service account roles, datasets, Kubernetes clusters
* Data Sources (e.g., Segment, SaaS webhooks): Subscribed events, destinations
* ETL tools (Airbyte, Fivetran, custom solutions): Sources, destinations, sync frequency, retry policies
* Transformation (dbt, sqlmesh, scheduled queries): commands, scheduling, retry policies, monitoring & alerting rules
* Reverse-ETL (Hightouch, Census, custom code): Sync schedules, destination configuration

As the number of workflows and data volume increase, Gorgias engineering teams tend to be spinning plates around workflow errors having to maintain complex logic across multiple heterogenous systems.

Embracing framework helps them have a more consistent stack: i.e. using Airbyte for data ingestion enforces writing connectors with a common structure. However, it was not enough to keep data flow logic versioned and maintainable.
Engineering and technical debts start to rise and it is more and more difficult to delegate part of the upstream scope (ingestion, transformation) to Data consumers (Data Analysts, Marketing, HR, Growth teams) as domains are mixed.

In this blog post, we'll dive into how Gorgias employs IaC principles for its data infrastructure using best-of-breed tools, including Kestra, Airbyte, dbt, Hightouch, and Terraform.

## The Control Plane: Data Orchestrator

While internal data analysis often doesn’t necessitate real-time processing, batch processing is optimally managed through orchestration tools, offering the capability to trigger and schedule tasks, much like a cron job.

Yet, some are tempted to consolidate all logic within highly customizable systems like Airflow. Doing ETL, transformation and reverse ETL all with custom code brings some drawbacks:

* Orchestration logic involving complex decorators can present a steep learning curve
* Intricacies of modularity and deployment strategies, such as building Docker images and creating PyPI packages, add complexity to an already sophisticated process
* Impact on the entire stack is not always straightforward when implementing changes, especially when updating shared components

Several months ago, Gorgias faced the limitations of embedded schedulers and embarked on a quest for a dedicated data orchestration tool.

Although many modern data tools integrate schedulers (Hightouch, Dbt cloud, Airbyte, GCP with Cloud Scheduler and scheduled queries), they often fall short in handling complex orchestration needs, such as retry policies, event-triggered workflows, and intricate dependencies.

Also, managing numerous execution logs across different tabs further underscored the need for a centralized orchestrator — a main control plane to streamline operations and enhance visibility.

## Looking For Existing Solutions

Gorgias's engineering team looked at several existing solutions that could fit their use cases. Especially Airflow and Dagster. However, both show some issues that could not answer their needs:

**Airflow**

* Hard to deploy: while managed versions like Cloud Composer on GCP are here to help, they are quite expensive and not easily scalable.
* Python Code: some Gorgias engineers were already quite familiar with Airflow, but their experience in maintaining a complete codebase and corresponding dependencies with it was far from ideal. While getting started with Python code is relatively easy, scaling it up requires a significant investment of effort and time.
* Poor UI: Airflow’s user interface has noticeably aged when compared to similar tools. The DAG view can be challenging for newcomers to navigate, and the overall user experience leaves much to be desired.
* Steep Learning Curve: Airflow encompasses a variety of concepts, and handling them in Python can occasionally be either overly complex or excessively wordy. Also, managing an Airflow installation in the long term can be very time-consuming, both in maintenance and being up to date with the latest versions or concepts.

**Dagster**

* Painful User Code Deployment (Open Source edition): while supporting different strategies for deployments, none of them were straightforward and needed to handle code locations deployment whereas in their case we just wanted a single codebase.
* Python Code: while Python is easy to understand, having to deal with many decorator functions is a bit too much from a framework that we initially anticipated to be simple and straightforward to maintain. Gorgias engineer's past experiences have taught them that keeping custom Python wrappers in good shape can become a daunting and arduous task over time…
* Steep learning curve: Dagster has a lot of different concepts and managing those in Python is sometimes too complicated or too verbose. On a long-term strategy, they had doubts regarding the integration of a less technical audience into such a framework.
* Declarative Data Orchestration with Kestra

Kestra offers an innovative way to deal with data pipelines based on YAML definition. Fulfilling their need for Infrastructure As Code pattern.
Other important aspects :

* Native integration with all the tools Gorgias is already using: Airbyte, BigQuery, PostgreSQL databases, dbt (CLI and cloud), GitHub, Slack, Hightouch, etc.
* A rich user interface to define tasks and manage flow executions
* Natively support Webhook trigger
* Terraform provider to allows versioning and modularity on top of Flows and thus introduces the best software engineering practices.
* Designed to scale and handle millions of flows in parallel, scaling it is made easy out of the box.
* Full control of data (self-hosted on Kubernetes)
* Easy to contribute to plugins.

With Kestra, the Gorgias team is addressing the following needs seamlessly :

* Triggering Airbyte sync followed by dbt transformations
* Have proper retry policies. Especially for dbt jobs, as they moved to capacity-based BigQuery pricing they can end up with resource exhaustion issues more often.
* Scheduling Hightouch sync at the end of dbt jobs for better freshness
* Trigger ETL python scripts with easy monitoring based on Webhook events
* Have a centralized place for all their logs
* Integrated UI allows you to define on-the-fly flows using a nice workflow editor and provide a large collection of blueprints :

![The workflow editor allows to define tasks fully using Kestra UI](/blogs/2024-01-11-gorgias/workflow_editor.png)

![UI provides details for each property available](/blogs/2024-01-11-gorgias/ui-editor.png)

![Blueprints are often good ways to start](/blogs/2024-01-11-gorgias/blueprint.png)

## Modularity with Kestra and Terraform

To provide modular development experience, the Gorgias team leverages Terraform modules and Kestra subflow patterns.

**Modules** are used as an abstraction layer for regular users to write flows (i.e. triggering an Airbyte sync followed by a dbt transformation) without having to worry about Kestra syntax, authentication, or connection details.

**Subflows** are used for generic tasks :
* They contain direct YAML and are declared within subflows/main.tf and define Inputs and Outputs clearly.
* Subflows can depend on each other, this dependency should be materialized by the depends_on field in subflows/main.tf. *Can be used by modules to hide some non-necessary complexity and to dry redundant tasks (Running a SQL query on a given PostgresDB)


![Flow definition to trigger an Airbyte sync and launch dbt command after completion](/blogs/2024-01-11-gorgias/terraform_module_def.png)

Logs are fully streamed from Airbyte to Kestra to provide a centralized platform for monitoring and debugging :

![Flow execution logs](/blogs/2024-01-11-gorgias/airbyte-dbt.png)

## Applying configuration using Kestra as a CI/CD tool

Maintaining a direct connection between external CI / CD tools like GitHub Action to the Kestra server service involves solutions like :
* Creating an ingress exposing the service: it involves using a VPN or an authentication mechanism to secure the system.
* Using port-forward during execution: adds complexity and it is not very standard

To be more efficient, Gorgias's team use Kestra as a CI / CD engine.
As they are applying configuration change on a self-hosted instance, it’s easier to run Terraform directly on their Kubernetes cluster.

With the dedicated Kestra Terraform plugin they can apply changes using the local k8s service endpoint using kube-DNS resolution.
This way, all is handled by Kestra.


![Terraform plan output in execution logs](/blogs/2024-01-11-gorgias/terrafrom_plan_logs.png)

![Topology of the “Terraform plan” flow](/blogs/2024-01-11-gorgias/terraform_topology.png)


## Conclusion

Declarative Data Engineering is rising as Data teams embrace SWE practices and Kestra got this point right allowing data practitioners to collaborate around a common tool.

Join the [Slack community](https://kestra.io/slack) if you have any questions or need assistance. Follow us on [Twitter](https://twitter.com/kestra_io) for the latest news. Check the code in our [GitHub repository](https://github.com/kestra-io/kestra) and give us a star if you like the project.