---
title: Outputs & Metrics from a script engine
---

Kestra can catch outputs and metrics from any scripting language. In the core, [Python](../../plugins/core/tasks/scripts/io.kestra.core.tasks.scripts.Python.md) and [Node](../../plugins/core/tasks/scripts/io.kestra.core.tasks.scripts.Node.md) plugins, inject a package to help you!

If you are using another language like [Bash](../../plugins/core/tasks/scripts/io.kestra.core.tasks.scripts.Bash.md), we don't provide any package to help you, but you can do it with the `echo` command on stdout.

## Script command

Kestra looks at every outputs on standard out (or standard err) and search for the special patterns `::{}::` or `{}` that allow to specify outputs and metrics as JSON objects:
- `{}` can be used for single line JSON objects.
- `::{}::` can be used for multi-line JSON objects.

Here is the representation of the output object, it's a map in which you can add any key/value pair:

```json5
"outputs": { // map of key value with all the outputs you want to send
    "my-key": "my-value",
    "my-list": [1, 2, 3] // you can use  json type (bool, array, map, â€¦)
}
```

Here is the representation of the metrics object, it's a list of JSON objects:
```json5
"metrics": [ // you can send multiple metrics at once
    {
        "name": "my-counter", // mandatory, the name of the metrics
        "type": "counter", // mandatory, "counter" or "timer"
        "value": 1.2, // mandatory (double), counter to add, or duration in seconds for timer
        "tags":{ // optional list of tags that will expose internal details
          "type":"read",
          "location":"eu"
        }
    }
]
```

## When to use metrics and when to use outputs?

### Use cases for outputs

Outputs are task run artifacts. They are generated as a result of a given task. If you want to track task run metadata across multiple executions of a flow, use `outputs` rather than `metrics`. 

Examples of metadata you may want to track as `outputs`: 

- the **number of rows** processed in a given task that you want to use in subsequent tasks validating the number of rows, 
- the **accuracy score** of a trained ML model in order to compare this result (output artifact) across multiple workflow executions, 
- other pieces of **metadata** you want to track across executions of a flow.

### Use cases for metrics

Metrics are intended to be used for custom numeric (metric type: `counter`) or duration (metric type: `timer`) attributes that you may want to visualize across tasks. Metrics are typically expressed as `double` data type numerical values.

Say, you are using the `EachParallel` task to process data across multiple partitions in parallel. You then want to visualize how many rows were processed in each partition and how long this process took. 

Here is a simple flow demonstrating how you can accomplish that:

```yaml
id: partitions
namespace: dev
description: Process partitions in parallel

tasks:
  - id: getPartitions
    type: io.kestra.core.tasks.scripts.Python
    inputFiles:
      main.py: |
        from kestra import Kestra
        partitions = [f"file_{nr}.parquet" for nr in range(1, 10)]
        Kestra.outputs({'partitions': partitions})

  - id: processPartitions
    type: io.kestra.core.tasks.flows.EachParallel
    value: '{{outputs.getPartitions.vars.partitions}}'
    tasks:
      - id: partition
        type: io.kestra.core.tasks.scripts.Python
        inputFiles:
          main.py: |
            import random
            import time
            from kestra import Kestra
            
            filename = '{{ taskrun.value }}'
            print(f"Reading and processing partition {filename}")
            nr_rows = random.randint(1, 1000)
            processing_time = random.randint(1, 20)
            time.sleep(processing_time)
            Kestra.counter('nr_rows', nr_rows, {'partition': filename})
            Kestra.timer('processing_time', processing_time, {'partition': filename})
```            

The above flow uses both metrics types: `counter` and `timer`, and the name of a partition is used as a `tag`. Run this flow and inspect the Metrics tab on the Execution page to see the difference between outputs (in this example the output is named: `partitions`) and metrics (here: the `nr_rows` and `processing_time`).


## Examples

```shell
# 1. send some outputs with different types
echo '::{"outputs":{"test":"value","int":2,"bool":true,"float":3.65}}::'

# 2. send a counter with tags
echo '::{"metrics":[{"name":"count","type":"counter","value":1,"tags":{"tag1":"i","tag2":"win"}}]}::'

# 3. send a timer with tags
echo '::{"metrics":[{"name":"time","type":"timer","value":2.12,"tags":{"tag1":"i","tag2":"destroy"}}]}::'
```
