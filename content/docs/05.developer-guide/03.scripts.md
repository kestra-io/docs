---
title: Python, R, Node.js and Shell Scripts
---

The [script plugin](https://github.com/kestra-io/plugin-scripts) allows you to orchestrate custom business logic written in `Python`, `R`, `Node.js`, `Powershell` and `Shell` scripts. By default, all these tasks run in individual Docker containers. You can optionally run those scripts in a local process or run them on a remote [worker group](../08.architecture.md#worker-group-ee). 


## The `Commands` and `Script` tasks

For each of these languages, Kestra provides two types of tasks: inline `Script` tasks, and `Commands` tasks triggered from the terminal based on the provided Docker image and optionally also based on a Git repository containing your code. 

1. The `Script` task is useful for scripts that can be written **inline** in a YAML flow configuration. They are best suited for simple automation tasks, e.g., if you want to run a single Python function fetching data from an API.
2. The `Commands` tasks can be combined with a `flows.WorkingDirectory` and `git.Clone` tasks to run custom scripts from Git in Docker containers without the overhead of packaging every script into a separate container image. This is possible thanks to the combination of a base `image` and custom `beforeCommands`. The next section explains how that works in detail.


The following table gives an overview of all script tasks and their configuration.

| Language   | Default image                | beforeCommands example                 | Script example               |
| ---------- | ---------------------------- | -------------------------------------- | ---------------------------- |
| Python     | python                       | pip install requests kestra            | print("Hello World!")        |
| R          | r-base                       | Rscript -e "install.packages('dplyr')" | print("Hello World!")        |
| Node.js    | node                         | npm install json2csv                   | console.log('Hello World!'); |
| Shell      | ubuntu                       | apt-get install curl                   | echo "Hello World!"          |
| Powershell | powershell | Install-Module -Name ImportExcel       | Write-Output "Hello World!"  |


**Full class names:**
- io.kestra.plugin.scripts.python.Commands
- io.kestra.plugin.scripts.python.Script
- io.kestra.plugin.scripts.r.Commands
- io.kestra.plugin.scripts.r.Script
- io.kestra.plugin.scripts.node.Commands
- io.kestra.plugin.scripts.node.Script
- io.kestra.plugin.scripts.shell.Commands
- io.kestra.plugin.scripts.shell.Script
- io.kestra.plugin.scripts.powershell.Commands
- io.kestra.plugin.scripts.powershell.Script


Each of those languages have a dedicated tag in the [Blueprints](../04.user-interface-guide/blueprints.md) catalog. Check available blueprints to get started with those tasks. The section below explains the recommended usage patterns and common examples.

--- 

## Recommended usage pattern: Scripts from Git in Docker

Below is a simple example of the recommended usage pattern: 
1. Start with a `flows.WorkingDirectory` task. This allows running multiple scripts in the same working directory, helping to **output results** and **pass data** between tasks when needed.
2. Use `git.Clone` as the first child task of a `flows.WorkingDirectory` task to ensure that your repository is cloned into an empty directory. You can provide a URL and authentication credentials to any Git-based system including GitHub, GitLab, BitBucket, CodeCommit, and more.
3. Use one of the `Commands` task (e.g. `python.Commands`) to specify which scripts or modules to run, while explicitly specifying the path within the Git repository. For instance, the command `python scripts/etl_script.py` ensures you run a Python script `etl_script.py` located in the `scripts` directory of the https://github.com/kestra-io/examples repository in the `main` branch. This repository is public, so the repository's `url` and a `branch` name is all that's required. If this repository were private, we would also had to specify `kestra-io` as the  `username` and add a [Personal Access Token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token) as the `password` property.
4. Specify your **base** Docker image using the `image` argument of the `docker` property. This image should include commonly used dependencies, for example `requests`, `pandas`, `sqlalchemy`, `scikit-learn`, etc. While you can have a dedicated image for each script, Kestra supports a much simpler Python dependency management pattern using a base image with shared dependencies and installing any custom dependencies needed for a particular script at runtime using the `beforeCommands` property.
5. The `beforeCommands` property is a list of arbitrary commands that can be *optionally* executed within the container before starting the script `commands`. If your cloned Git repository includes `requirements.txt` file, you can use `pip install -r requirements.txt` to install the required `pip` packages. 
	1. The additional `warningOnStdErr` boolean flag can be set to `false` to avoid setting the task to a WARNING state when the installation process emits warnings to the standard error. By default, warnings during e.g. `pip` package installation will set the task state to a `WARNING` state to give you more transparency about the entire process. 
	2. Redirecting the installation output to `/dev/null` in the command `pip install faker > /dev/null`ensures that the (*usually verbose*) output emitted during installation is not captured within your task logs. 
6. The `commands` is a list of commands that will be executed sequentially. This setup provides a high degree of flexibility as it allows you to execute parametrized scripts, custom CLI applications, etc. 
7. The `LocalFiles` task can be used to output a file generated by a script. Here, the [generate_order.py](https://github.com/kestra-io/examples/blob/main/scripts/generate_orders.py#L24) script creates a file `orders.csv`. When you reference this file in the `outputs` property, it will be available for download from the **Outputs** tab on the Executions page. You'll be able to retrieve that file in downstream tasks using the syntax `{{outputs.taskId.uris['outputFileName.fileExtension']}}`, which in this example would be  `{{outputs.outputCsv.uris['orders.csv']}}`.  In this specific flow, we use that file in a downstream task that loads that file to an S3 bucket. 


```yml
id: pythonCommandsExample
namespace: dev

tasks:
  - id: wdir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: cloneRepository
        type: io.kestra.plugin.git.Clone
        url: https://github.com/kestra-io/examples
        branch: main

      - id: gitPythonScripts
        type: io.kestra.plugin.scripts.python.Commands
        warningOnStdErr: false
        runner: DOCKER
        docker:
          image: ghcr.io/kestra-io/pydata:latest
        beforeCommands:
          - pip install faker > /dev/null
        commands:
          - python scripts/etl_script.py
          - python scripts/generate_orders.py
      
      - id: outputFile
        type: io.kestra.core.tasks.storages.LocalFiles
        outputs:
          - orders.csv

  - id: loadCsvToS3
    type: io.kestra.plugin.aws.s3.Upload
    accessKeyId: "{{secret('AWS_ACCESS_KEY_ID')}}"
    secretKeyId: "{{secret('AWS_SECRET_ACCESS_KEY')}}"
    region: eu-central-1
    bucket: kestraio
    key: stage/orders.csv
    from: "{{outputs.outputFile.uris['orders.csv']}}"

```


> **Why Git? Why not "just" use a locally stored file?** We believe that using Git already during local development of your data pipelines allows a more robust engineering workflow and makes it easier to move from local development to production. Before using any script in Kestra, you should first thoroughly test that script in your favorite IDE and only use Kestra to orchestrate it. For local development, we recommend using a feature branch or a `dev`/`develop` branch, rather than mounting files to Docker containers.  However, if you don't want to use Git, the section below demonstrates the usage of inline `Script` tasks.


---

## Simple usage pattern: Inline Scripts in Docker

In contrast to the `Commands` tasks, the `Script` task doesn't require using Git to store your script code. Instead, you define it inline in your YAML workflow definition along with any other configuration. 

Here is a simple example illustrating how to use that pattern: 
1. Start with a `flows.WorkingDirectory` task if you want to provide additional **input** files or when your task needs to **output** files using the `LocalFiles` task. This example uses a SQL file as input.
2. Use one of the `Script` tasks (e.g. `python.Script`) and paste your Python/R/Node.js/Shell script as a multiline string into the `script` property. 
3. You can *optionally* overwrite the **base** Docker image using the `image` argument of the `docker` property. This works exactly the same way as in the `Commands` tasks. 
4. You can also *optionally* use the `beforeCommands` property to install libraries used in your inline script. Here, we use the command `pip install requests kestra` to install two new `pip` packages not available in the base image `python:3.11-slim`. 
5. The `LocalFiles` task can be used to output a file generated as a result of a script. This script outputs a JSON file and a downstream task loads that file as a document to a MongoDB collection. 

```yaml
id: apiJSONtoMongoDB
namespace: dev

tasks:
  - id: wdir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: demoSQL
        type: io.kestra.core.tasks.storages.LocalFiles
        inputs: 
          query.sql: |
            SELECT sum(total) as total, avg(quantity) as avg_quantity
            FROM sales;

      - id: inlineScript
        type: io.kestra.plugin.scripts.python.Script
        runner: DOCKER
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install requests kestra > /dev/null
        warningOnStdErr: false
        script: | 
          import requests
          import json
          from kestra import Kestra

          with open('query.sql', 'r') as input_file:
              sql = input_file.read()

          response = requests.get("https://api.github.com")
          data = response.json()

          with open("output.json", "w") as output_file:
              json.dump(data, output_file)
          
          Kestra.outputs({'receivedSQL': sql, 'status': response.status_code})

      - id: jsonFiles
        type: io.kestra.core.tasks.storages.LocalFiles
        outputs:
          - output.json

  - id: loadToMongoDB
    type: io.kestra.plugin.mongodb.Load
    connection:
      uri: mongodb://host.docker.internal:27017/
    database: local
    collection: github
    from: "{{outputs.jsonFiles.uris['output.json']}}"

```

---

## When to use `Script` over `Commands`?

The `Script` pattern has several **advantages**:
- **Simplicity**: you don't need to commit files to a Git repository. The script code is stored and **versioned** using Kestra's revision history along with your orchestration code.
- Easier **integration with the templating engine**: when the entire workflow is defined in one configuration file, it can be easier to see dependencies and use templating to share variable definitions, pass outputs to downstream tasks, etc.

However, the `Script` tasks also have some **disadvantages** as compared to the `Commands` tasks:
- Adding programming language code into a configuration file such as YAML makes that code more difficult to read, test and review. It lacks the syntax highlighting and testability you would otherwise get from your IDE when writing e.g. Python script in a `.py` file rather than pasting those inline into a `.yml` config.
- The `Script` task is not meant for complex use cases where your business logic is comprised of multiple files importing classes and functions from other modules, etc.

Overall, we recommend using `git.Clone` and `Commands` tasks for advanced production workloads. However, the `Script` tasks offer a great alternative for simple processes.

---

## Runners

By default all scripting tasks run in isolated containers using the `DOCKER` process. Setting the `runner` property to `PROCESS` will execute your task in a local process on the Worker without relying on Docker for container isolation.

### runner: DOCKER

This is the default option for all script tasks. There are many arguments that can be provided here, including credentials to private Docker registries:

```yaml
id: pythonInContainer
namespace: dev

tasks:
  - id: wdir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: cloneRepository
        type: io.kestra.plugin.git.Clone
        url: https://github.com/kestra-io/examples
        branch: main

      - id: gitPythonScripts
        type: io.kestra.plugin.scripts.python.Commands
        warningOnStdErr: false
        commands:
          - python scripts/create_messy_dataset.py
        runner: DOCKER
        docker:
          image: annageller/kestra:latest
          config: |
            {
              "auths": {
                  "https://index.docker.io/v1/": {
                      "username": "annageller",
                      "password": "{{secret('DOCKER_PAT')}}"
                  }
              }
            }
      - id: output
        type: io.kestra.core.tasks.storages.LocalFiles
        outputs:
          - "*.csv"
          - "*.parquet"
```


### runner: PROCESS

The `PROCESS` runner is useful if your Kestra instance is running [locally without Docker](../09.administrator-guide/02.deployment/03.manual.md) and you want to access your local files and environments e.g. locally configured Conda virtual environments. 

Running scripts in a local process are especially powerful when using [remote Worker Groups](../08.architecture.md#worker-group-ee). The example below ensures that a script will be picked up only by Kestra workers that have been started with the key `gpu`, effectively delegating scripts that require demanding computational requirements to the right server, rather than running them directly in a local container:

```yaml
id: gpuTask
namespace: dev

tasks:
  - id: hello
    type: io.kestra.plugin.scripts.python.Commands
    runner: PROCESS
    commands:
		- python ml_on_gpu.py
    workerGroup:
      key: gpu
```    


---

## Installing dependencies using `beforeCommands`

There are several ways of installing custom packages for your workflows. While you could bake all your package dependencies into a custom container, often it's convenient to install a couple of additional packages at runtime without having to build additional images. The `beforeCommands` can be used for that purpose.

### pip install package

Here is a simple example installing some `pip` packages:

```yaml
id: pip
namespace: dev

tasks:
  - id: wdir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: inlineScript
        type: io.kestra.plugin.scripts.python.Script
        runner: DOCKER
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install requests kestra > /dev/null
        warningOnStdErr: false
        script: | 
          import requests
          import kestra

          print(requests.__version__)
          print([i for i in dir(kestra.Kestra) if not i.startswith("_")])
```


### pip install -r requirements.txt

This example clones a Git repository that contains a `requirements.txt` file. The script task uses `beforeCommands` to install those packages. We then list recently installed packages to validate that this process works as expected:

```yaml
id: pythonRequirementsFile
namespace: dev

tasks:
  - id: wdir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: cloneRepository
        type: io.kestra.plugin.git.Clone
        url: https://github.com/kestra-io/examples
        branch: main

      - id: printRequirements
        type: io.kestra.plugin.scripts.shell.Commands
        runner: PROCESS
        commands:
          - cat requirements.txt

      - id: listRecentlyInstalledPackages
        type: io.kestra.plugin.scripts.python.Commands
        warningOnStdErr: false
        runner: DOCKER
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install -r requirements.txt > /dev/null
        commands:
          - ls -lt $(python -c "import site; print(site.getsitepackages()[0])") | head -n 20
```


## Using our prebuilt images

Many tasks in data engineering require working with fairly standardized tasks such as: 

- processing data with pandas 
- transforming data with dbt core using the dbt adapter for your specific data warehouse
- making API calls with requests, etc. 

To solve those common challenges, the [kestra-io/examples repository](https://github.com/orgs/kestra-io/packages?repo_name=examples) provides several public Docker images with the latest versions of those common packages. 

Many blueprints rely on those images. Here is a simple example using the `rdata` image provided by Kestra to analyze the built-in `mtcars` dataset using `dplyr` and `arrow` libraries:

```yaml
id: rCars
namespace: dev

tasks:
  - id: wdir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: r
        type: io.kestra.plugin.scripts.r.Script
        warningOnStdErr: false
        runner: DOCKER
        docker:
          image: ghcr.io/kestra-io/rdata:latest
        script: |
          library(dplyr)
          library(arrow)

          data(mtcars) # Load mtcars data
          print(head(mtcars))

          final <- mtcars %>%
            summarise(
              avg_mpg = mean(mpg),
              avg_disp = mean(disp),
              avg_hp = mean(hp),
              avg_drat = mean(drat),
              avg_wt = mean(wt),
              avg_qsec = mean(qsec),
              avg_vs = mean(vs),
              avg_am = mean(am),
              avg_gear = mean(gear),
              avg_carb = mean(carb)
            ) 
          final %>% print()
          write.csv(final, "final.csv")

          mtcars_clean <- na.omit(mtcars) # remove rows with NA values
          write_parquet(mtcars_clean, "mtcars_clean.parquet")

      - id: outputs
        type: io.kestra.core.tasks.storages.LocalFiles
        outputs:
          - "*.csv"
          - "*.parquet"
```

We use R scripts as an example here, because R libraries are extremely time-consuming to install. Technically, you could install custom R packages at runtime as follows:

```yaml
id: rCars
namespace: dev

tasks:
  - id: wdir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: r
        type: io.kestra.plugin.scripts.r.Script
        warningOnStdErr: false
        runner: DOCKER
        docker:
          image: ghcr.io/kestra-io/rdata:latest
        beforeCommands: 
	        - Rscript -e "install.packages(c('dplyr', 'arrow'))" > /dev/null 2>&1
```


But it may take up to 30 minutes to install, depending on the packages you install.

Prebuilt images such as `ghcr.io/kestra-io/rdata:latest` help you iterate much faster on simple R scripts. 

## Support for additional languages such as Rust via `shell.Commands` and Docker runner

The `Commands` scripts allow you to run arbitrary commands based on your custom Docker images. This means that you can also use other languages as long as their dependencies can be packaged into a Docker image and their execution can be triggered from a `Shell` command line. 

Here is an example flow using a Rust image based on this ETL toy project:

```yaml
id: rustFlow
namespace: dev
tasks:
  - id: wdir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: rust
        type: io.kestra.plugin.scripts.shell.Commands
        commands:
          - etl
        runner: DOCKER
        docker:
          image: ghcr.io/kestra-io/rust:latest

      - id: downloadFiles
        type: io.kestra.core.tasks.storages.LocalFiles
        outputs:
          - "*.csv"
```

That image is public, so you can give it a try.

---

## The `Git` plugin

To use the `git.Clone` task in your flow, make sure to add it as the first child task of the `WorkingDirectory` task. Otherwise, you’ll get an error: `Destination path "1II20XZRjzdbbYnQ4htqea" already exists and is not an empty directory`. This happens because you can only clone a GitHub repository into an empty working directory. Adding the `git.Clone` task directly as the first child task of the `WorkingDirectory` task will ensure that you clone your repository into an empty directory before any other task would generate any outputs.

Typically, you would want to use `git.Clone`  with a private GitHub repository. Make sure to generate your access token first and provide it on the `password` property.

Below you can find links to instructions on how to generate an access token for the relevant Git platform:

- [GitHub](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens)
- [Gitlab](https://docs.gitlab.com/ee/user/profile/personal_access_tokens.html)
- [Bitbucket](https://support.atlassian.com/bitbucket-cloud/docs/create-a-repository-access-token/)
- [AWS CodeCommit](https://docs.aws.amazon.com/codecommit/latest/userguide/auth-and-access-control.html)
- [Azure DevOps](https://learn.microsoft.com/en-us/azure/devops/organizations/accounts/use-personal-access-tokens-to-authenticate?view=azure-devops&tabs=Windows)

---

## Why and when to use the `WorkingDirectory` task

By default, all Kestra tasks are stateless. If one task generates files, those files won’t be available in downstream tasks unless they are persisted in internal storage. Upon each task completion, the temporary directory for the task is purged. This behavior is generally useful as it keeps your environment clean and dependency free, and it avoids potential privacy or security issues when exposing some data generated by a task to other processes.

Despite the benefits of the stateless execution, in certain scenarios, statefulness is desirable. Imagine that you want to execute several Python scripts, and each of them generates some output data. Another script combines that data as part of an ETL/ML process. Executing those related tasks in the same working directory and sharing state between them is helpful for the following reasons:
- you can clone your entire GitHub branch with multiple modules and configuration files needed to run several scripts,
- you can execute those scripts sequentially on the same worker, minimizing latency,
- outputs of each task are directly available to other tasks without having to persist them within the internal storage.

The `WorkingDirectory` task has been designed to address all of these challenges. It allows you to clone a Git repository (or retrieve needed input files in other ways), run multiple tasks sequentially in the same working directory and reuse the same file system's working directory across multiple tasks. All tasks within the `WorkingDirectory` can use output files from previous tasks without having to use the `{{outputs.taskId.outputAttribute}}` syntax.

---

## The `LocalFiles` task to manage `inputs` and `outputs` for your script code

The `LocalFiles` task is meant to be used inside of the `WorkingDirectory` task. 

It allows you to **output files** generated in a script and **add new files inline** within the YAML workflow definition. The examples from the previous section show how you can use **inputs** and **outputs**. The sub-sections below provide additional explanation and even more examples. 

### Managing input files: `LocalFiles.inputs`

The `inputs` property can be used to add input files that might be needed for your script. Imagine that your Python script is executing a SQL query. Perhaps you would want to provide that query as a separate input file. Or maybe you want to add a custom `requirements.txt` file that contains exact pip package dependencies:

```yaml
      - id: pip
        type: io.kestra.core.tasks.storages.LocalFiles
        inputs:
          requirements.txt: |
			kestra>=0.1.2
			pandas>=1.2.3
			requests>=2.3.4

	  - id: pythonScript
        type: io.kestra.plugin.scripts.python.Script
        runner: DOCKER
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install -r requirements.txt
```

Providing such input configuration files is one of the main use cases for using `inputs` property within the `LocalFiles`. 

Another use case for input files is when your custom scripts need input coming from other tasks or triggers. Consider the following example flow that runs when a new object with the prefix `"raw/"` arrives in the S3 bucket `"declarative-orchestration"`:

```yml
id: s3TriggerCommands
namespace: blueprint
description: process CSV file from S3 trigger

tasks:
  - id: wdir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: cloneRepo
        type: io.kestra.plugin.git.Clone
        url: https://github.com/kestra-io/examples
        branch: main

      - id: local
        type: io.kestra.core.tasks.storages.LocalFiles
        inputs:
          data.csv: "{{ trigger.objects | jq('.[].uri') | first }}"

      - id: python
        type: io.kestra.plugin.scripts.python.Commands
        description: this script expects to. receive a file `data.csv` as input from S3 trigger
        runner: DOCKER
        docker:
          image: ghcr.io/kestra-io/pydata:latest
        warningOnStdErr: false
        commands:
          - python scripts/clean_messy_dataset.py

      - id: output
        type: io.kestra.core.tasks.storages.LocalFiles
        outputs:
          - *.csv
          - *.parquet

triggers:
  - id: waitForS3object
    type: io.kestra.plugin.aws.s3.Trigger
    bucket: declarative-orchestration
    region: eu-central-1
    maxKeys: 1
    interval: PT1S
    filter: FILES
    action: MOVE
    prefix: raw/
    moveTo:
      key: archive/raw/
    accessKeyId: "{{secret('AWS_ACCESS_KEY_ID')}}"
    secretKeyId: "{{secret('AWS_SECRET_ACCESS_KEY')}}"

```

Because the `LocalFiles` is a dedicated task independent of the Python task, it can be used to pass the S3 object key (*downloaded to Kestra's internal storage by the S3 trigger*) as a local file to any downstream task. Note how we didn't have to hardcode any Kestra-specific code in the Python script from GitHub. That script remains pure Python that you can run anywhere. Kestra's trigger, orchestration and infrastructure configuration are defined only in this single YAML configuration. 

This separation of concerns (*i.e. not mixing orchestration and business logic*) makes your code easier to test and keeps your business logic vendor-agnostic. 


### Managing output files: `LocalFiles.outputs`

Using the previous example, note how the `LocalFiles` can also be used to output any file. This script from GitHub outputs a file named `clean_dataset.csv` but you don't have to hardcode that specific file name. Instead, you can define that you want to output any CSV or any Parquet file generated by that script using a simple [Glob](https://en.wikipedia.org/wiki/Glob_(programming)) expression: 

```yaml
      - id: output
        type: io.kestra.core.tasks.storages.LocalFiles
        outputs:
          - "*.csv"
          - "*.parquet"
```


---

## How to emit `outputs` and `metrics` from script tasks

This section talks about different kind of outputs than the last section. The `LocalFiles.outputs` is about sending files generated in a script to Kestra's internal storage so that they can be used in downstream tasks or exposed as downloadable artifacts. 

However,  `outputs` can often also be simple key-value pairs that contain metadata generated in your scripts. Many tasks in Kestra generate outputs. You can inspect which outputs are generated by each task or trigger from the respective plugin documentation. For instance, follow [this plugin documentation link](https://kestra.io/plugins/plugin-fs/tasks/http/io.kestra.plugin.fs.http.download#outputs) to see the outputs generated by the HTTP Download task. Once the flow is executed, the Outputs tab list all these output metadata as key-value pairs. Run the example below to see it in action:

```yml
id: download
namespace: dev
tasks:
  - id: http
    type: io.kestra.plugin.fs.http.Download
    uri: https://raw.githubusercontent.com/kestra-io/examples/main/datasets/orders.csv    
```

This example automatically generates output because the task of downloading a file from a URL is fairly standardized. However, in your custom script, you can decide what metadata you want to send to Kestra to make that metadata visible in the UI.

The example below shows how you can add simple key-value pairs in your script to send custom metrics and outputs: 

```python
from kestra import Kestra

Kestra.outputs({'data': data, 'nr': 42})
Kestra.counter('nr_rows', len(df), tags={'partition': filename})  
Kestra.timer('processing_time', duration, tags={'partition': filename})
```

You can see that `Kestra.outputs({"key": "value"})` takes a dictionary of key-value pairs, while the metrics such as **Counter** and **Timer** take the metric name, metric value and a dictionary of tags as positional arguments: 
- `Kestra.counter("countable_int_metric_name", 42, tags={"key": "value"})`
- `Kestra.timer("countable_double_metric_name", 42.42, tags={"key": "value"})`

Node.js follows the same syntax for sending outputs and metrics as in Python, but R and Powershell are yet to be added. 

---

## When to use metrics and when to use outputs?



### Use cases for outputs 

Outputs are task-run artifacts. They are generated as a **result** of a given task. If you want to track task-run metadata across multiple executions of a flow, and this metadata is of an arbitrary data type (it might be a string, integer, ...), use `outputs` rather than `metrics`.

Examples of metadata you may want to track as `outputs`:

- the **number of rows** processed in a given task that you want to use in subsequent tasks validating the number of rows,
- the **accuracy score** of a trained ML model in order to compare this result (output artifact) across multiple workflow executions,
- other pieces of **metadata** you want to track across executions of a flow (e.g. a dataset name used within a Python ETL script).

Outputs can be used to pass data between tasks. One task can generate some outputs and other task can use that value:

```yml
id: outputsInputs
namespace: dev
tasks:
    - id: passOutput
      type: io.kestra.core.tasks.debugs.Return
      format: "hello world!"
    - id: takeInput
      type: io.kestra.core.tasks.debugs.Return
      format: "data from previous task - {{outputs.passOutput.value}}"
```

### Use cases for metrics 

Metrics are intended to track custom numeric (metric type: `counter`) or duration (metric type: `timer`) attributes that you can visualize across task runs and flow executions. Metrics are expressed as `integer` or  `double` data type numerical values.

Say, you are using the `EachParallel` task to process data across multiple partitions in parallel. You then want to track how many rows were processed in each partition and how long this process took.

Here is a flow demonstrating how you can accomplish that:
```yml
id: pythonPartitionsMetrics
namespace: blueprint
description: Process partitions in parallel

tasks:
  - id: getPartitions
    type: io.kestra.plugin.scripts.python.Script
    runner: DOCKER
    docker:
      image: ghcr.io/kestra-io/pydata:latest
    script: |
      from kestra import Kestra
      partitions = [f"file_{nr}.parquet" for nr in range(1, 10)]
      Kestra.outputs({'partitions': partitions})

  - id: processPartitions
    type: io.kestra.core.tasks.flows.EachParallel
    value: '{{outputs.getPartitions.vars.partitions}}'
    tasks:
      - id: partition
        type: io.kestra.plugin.scripts.python.Script
        runner: DOCKER
        docker:
          image: ghcr.io/kestra-io/pydata:latest
        script: |
          import random
          import time
          from kestra import Kestra

          filename = '{{ taskrun.value }}'
          print(f"Reading and processing partition {filename}")
          nr_rows = random.randint(1, 1000)
          processing_time = random.randint(1, 20)
          time.sleep(processing_time)
          Kestra.counter('nr_rows', nr_rows, {'partition': filename})
          Kestra.timer('processing_time', processing_time, {'partition': filename})
```

The above flow uses both metrics types: `counter` and `timer`, and the partition name is used as a `tag`.

Run the above flow example and inspect both the **Metrics** and **Outputs** tabs on the **Execution** page of that flow to see the difference between outputs (in this example, the output is named: `partitions`) and metrics (here: the `nr_rows` and `processing_time`).


### Examples for Shell tasks
```shell
# 1. send some outputs with different types
echo '::{"outputs":{"test":"value","int":2,"bool":true,"float":3.65}}::'

# 2. send a counter with tags
echo '::{"metrics":[{"name":"count","type":"counter","value":1,"tags":{"tag1":"i","tag2":"win"}}]}::'

# 3. send a timer with tags
echo '::{"metrics":[{"name":"time","type":"timer","value":2.12,"tags":{"tag1":"i","tag2":"destroy"}}]}::'
```

